{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os   # for path operations\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config GPU env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15916943652870412808\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3168377241\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17564788919795856927\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "# Device check\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Def NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=40\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "checkpointer=ModelCheckpoint(filepath=\"weights.hdf5\", save_best_only=True, verbose =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 1D convnet based on the practice lesson for time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_1d_convnet(window_size, filter_length, nb_input_series=1, nb_outputs=3,nb_filter=4):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=nb_filter, kernel_size=filter_length, activation='relu', \n",
    "                   input_shape=(window_size,nb_input_series)))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(filters=nb_filter, kernel_size=filter_length, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_outputs,activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input and output format from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_timeseries(timeseries,yindex, window_size, filter_length, nb_filter , epochs):\n",
    "    #timeseries = np.atleast_2d(timeseries)\n",
    "    if timeseries.shape[0] == 1:\n",
    "        timeseries = timeseries.T\n",
    "    nb_samples, nb_series = timeseries.shape\n",
    "    nb_inputs = nb_series - 11 #only 12 inputs (previous x,y,z position is an input now)\n",
    "    #nb_outputs = nb_series - 9 #only 9 outputs\n",
    "    nb_outputs=3\n",
    "  \n",
    "    model= make_1d_convnet(window_size=window_size, filter_length=filter_length, nb_input_series=nb_inputs,\n",
    "                        nb_outputs=nb_outputs,nb_filter=nb_filter)\n",
    "    model.summary()\n",
    "    \n",
    "    # def make_timeseries_instances\n",
    "    timeseries = np.asarray(timeseries)\n",
    "    assert 0 < window_size < timeseries.shape[0] , \"Out of range 0 < {} < {}\".format(window_size,timeseries.shape[0])\n",
    "    # nb_input defines the number of input time series that we use to predict outputs\n",
    "    X = np.atleast_3d(np.array([timeseries[start:start+window_size,:nb_inputs] for start in range(0,timeseries.shape[0]-window_size)]))\n",
    "    # We have 3 output signal: x, y and z position\n",
    "    # y = timeseries[window_size:,-9:-6]\n",
    "    # We have 1 output signal: orient\n",
    "    y = timeseries[window_size:,-11:-3]\n",
    "  \n",
    "    test_size = int(0.3  * nb_samples)\n",
    "    valid_size = int(0.2 * nb_samples)\n",
    "    X_train, X_valid, X_test = X[:-(test_size+valid_size),:], X[-(test_size+valid_size):-test_size,:], X[-test_size:,:]\n",
    "    y_train, y_valid, y_test = y[:-(test_size+valid_size),:], y[-(test_size+valid_size):-test_size,:], y[-test_size:,:]\n",
    "    \n",
    "    # Standardize input variables for train, valid and test data\n",
    "    for inpt in range(X.shape[1]):\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train[:,inpt])\n",
    "        X_train[:,inpt] = scaler.transform(X_train[:,inpt])\n",
    "        X_valid[:,inpt] = scaler.transform(X_valid[:,inpt])\n",
    "        X_test[:,inpt] = scaler.transform(X_test[:,inpt])\n",
    "    \n",
    "    model.fit(X_train,y_train, epochs = epochs, batch_size = 16, validation_data = (X_valid, y_valid),\n",
    "              verbose = 2, callbacks = [checkpointer,early_stopping])\n",
    "  \n",
    "    preds=model.predict(X_test)\n",
    "    targets = y_test\n",
    "    \n",
    "    print(preds.shape)\n",
    "    print(targets.shape)\n",
    "    \n",
    "    # Plot predicted x vs target x\n",
    "    plt.figure(0)\n",
    "    plt.plot(preds[:,0],color='green')\n",
    "    plt.plot(targets[:,0],color='red')\n",
    "    \n",
    "    # Plot predicted z vs target z\n",
    "    plt.figure(1)\n",
    "    plt.plot(preds[:,2],color='blue')\n",
    "    plt.plot(targets[:,2],color='yellow')\n",
    "    \n",
    "    # Plot the predicted route vs original route on x-z plane\n",
    "    plt.figure(2)\n",
    "    plt.plot(preds[:,0],preds[:,2])\n",
    "    plt.plot(targets[:,0],targets[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_meas = '..\\\\..\\\\DATA\\\\RAW\\\\Measure_02' # Location folder of the measurement files\n",
    "file_list = os.listdir(lock_meas)           # Make a list out of the files in the folder\n",
    "nb_meas = len(file_list)/4                  # Every measurement contains 4 file (IMU, MoCap, Robotino, NFO)\n",
    "\n",
    "meas_list = []                              # Make a list containing the independent measurements\n",
    "for FILE_NAME in file_list:\n",
    "    meas_list.append(FILE_NAME[0:22])       # The unique ID is the time, the first 22 character\n",
    "meas_list = list(set(meas_list))\n",
    "\n",
    "# Load in one favourite measurement block\n",
    "meas_id = 4\n",
    "\n",
    "# Syncronise the measurement files from MoCap and IMU\n",
    "if meas_id>(nb_meas-1):\n",
    "    meas = 0\n",
    "meas_date = str(meas_list[meas_id])\n",
    "\n",
    "# Load the IMU csv data file\n",
    "imu_data = pd.read_csv(lock_meas + '\\\\' + meas_date + 'IMU.txt',\n",
    "                               sep='\\t',\n",
    "                               decimal=',',\n",
    "                               names=['time', 'acc0', 'acc1', 'acc2', 'gyro0', 'gyro1', 'gyro2', 'mag0', 'mag1', 'mag2'])\n",
    "\n",
    "# Load the MoCap csv data file\n",
    "mocap_data = pd.read_csv(lock_meas + '\\\\' + meas_date + 'MoCap.txt',\n",
    "                                 sep='\\t',\n",
    "                                 decimal=',',\n",
    "                                 names=['time', 'x', 'y', 'z', 'tracked', 'beta', 'Qx', 'Qy', 'Qz', 'Qw'])\n",
    "\n",
    "# Merge the two data file to synronise them. In both dataset there are some data row that can't be matched, this data will\n",
    "# be trown away\n",
    "data = pd.merge(imu_data, mocap_data, on=['time'], how='inner')\n",
    "\n",
    "# ===== Filters ======\n",
    "\n",
    "# When the magneto sensors values are [0, 0, 0] that is a false value. These rows are deleted.\n",
    "# When tacked is 0, it is indicates that the MoCap data is invalid. These rows are deleted too.\n",
    "df = data[~(data[['mag0','mag1','mag2','tracked']] == 0).any(axis=1)]\n",
    "# From now on the tracked column can be deleted, because it contains only 1s.\n",
    "del df['tracked']\n",
    "# Add to more columns to make a fluent orientation function (orient) and a delta time (deltat)\n",
    "df = df.assign(orient=0)\n",
    "df = df.assign(deltat=0)\n",
    "df = df.assign(level=0)\n",
    "# Convert the data frame into numpy array\n",
    "dfarray = np.array(df)\n",
    "# Create a lookup table to make the use of this matrix more readable.\n",
    "time   = 0\n",
    "acc0   = 1\n",
    "acc1   = 2\n",
    "acc2   = 3\n",
    "gyro0  = 4\n",
    "gyro1  = 5\n",
    "gyro2  = 6\n",
    "mag0   = 7\n",
    "mag1   = 8\n",
    "mag2   = 9\n",
    "x      = 10\n",
    "y      = 11\n",
    "z      = 12\n",
    "beta   = 13\n",
    "Qx     = 14\n",
    "Qy     = 15\n",
    "Qz     = 16\n",
    "Qw     = 17\n",
    "orient = 18\n",
    "deltat = 19\n",
    "level  = 20\n",
    "\n",
    "# Calculate the delta time between two valid measurements\n",
    "for i in range(dfarray.shape[0]):\n",
    "    if (i-1)>-1:\n",
    "        dfarray[i,deltat] = dfarray[i,time]-dfarray[i-1,time]\n",
    "\n",
    "# Creating a more fluent orientation function\n",
    "level = 0\n",
    "for i in range(dfarray.shape[0]):\n",
    "    if (i-1)>-1:\n",
    "        if (dfarray[i,beta]-dfarray[i-1,beta])>355:\n",
    "            level = level - 1\n",
    "        if (dfarray[i,beta]-dfarray[i-1,beta])<-355:\n",
    "            level = level + 1\n",
    "        else:\n",
    "            level = level   \n",
    "        dfarray[i, orient] = level*360 + dfarray[i, beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D convolution\n",
    "window_size = 20\n",
    "filter_length = 5\n",
    "nb_filter = 4\n",
    "\n",
    "#Train config\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results:\n",
    "\n",
    "* First plot: x position (predictions: green, target: red)\n",
    "\n",
    "* Second plot: z position (predictions: blue, target: yellow)\n",
    "\n",
    "* Third plot: route of the robot on the x-z plane (predictions: blue, target: orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16, 4)             204       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8, 4)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 4, 4)              84        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 2, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 315\n",
      "Trainable params: 315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (3,) but got array with shape (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-de2cd66b2d28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevalute_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfarray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilter_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_filter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-c16997650793>\u001b[0m in \u001b[0;36mevalute_timeseries\u001b[1;34m(timeseries, yindex, window_size, filter_length, nb_filter, epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     model.fit(X_train,y_train, epochs = epochs, batch_size = 16, validation_data = (X_valid, y_valid),\n\u001b[1;32m---> 37\u001b[1;33m               verbose = 2, callbacks = [checkpointer,early_stopping])\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_STORAGE\\Programs\\Anaconda3\\envs\\work\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_STORAGE\\Programs\\Anaconda3\\envs\\work\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\0_STORAGE\\Programs\\Anaconda3\\envs\\work\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (3,) but got array with shape (8,)"
     ]
    }
   ],
   "source": [
    "evalute_timeseries(dfarray,orient,window_size,filter_length,nb_filter,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original measurement on x-z plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dfarray[:,-9],dfarray[:,-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
